from aws_cdk import (
    Stack,
    aws_s3 as s3,
    aws_kinesis as kinesis,
    aws_iam as iam,
    RemovalPolicy,
    CfnOutput
)
from constructs import Construct

class CdkInfraStack(Stack):

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # 1. Delta Lake S3 Bucket (Data Lake Storage)
        # This bucket will store all the Delta Lake files (raw, processed, and checkpoints).
        delta_lake_bucket = s3.Bucket(
            self, "DeltaLakeBucket",
            # The bucket name will be generated by CDK for uniqueness, but we output it below.
            versioned=False,
            block_public_access=s3.BlockPublicAccess.BLOCK_ALL,
            removal_policy=RemovalPolicy.DESTROY, # WARNING: good for this PoC, not for production
            auto_delete_objects=True # Automatically deletes files when the bucket is destroyed
        )

        # 2. Kinesis Data Stream
        # This is the real-time ingestion point for the incoming simulated data.
        iot_stream = kinesis.Stream(
            self, "IoTSensorDataStream",
            stream_name="iot-sensor-data-stream",
            shard_count=1 
        )
        iot_stream.apply_removal_policy(RemovalPolicy.DESTROY) # WARNING: good for this PoC, not for production
        
        # 3. IAM Role for Databricks Cluster EC2 Instances
        # Databricks clusters run on EC2 instances, which need to assume this role.
        databricks_role = iam.Role(
            self, "DatabricksS3KinesisRole",
            assumed_by=iam.ServicePrincipal('ec2.amazonaws.com'), 
            description="Role for Databricks cluster to access S3 Delta Lake and Kinesis Stream."
        )

        # Grant the role permissions to access the kinesis stream (READ)
        iot_stream.grant_read(databricks_role)

        # Grant the role read/write access to the S3 bucket and all its contents.
        # This is what enables Databricks to write Delta files and checkpoints.
        delta_lake_bucket.grant_read_write(databricks_role)
        
        # 4. Create IAM Instance Profile
        # Databricks requires an Instance Profile to be attached to the EC2 instances in the cluster.
        instance_profile = iam.CfnInstanceProfile(
            self, "DatabricksInstanceProfile",
            instance_profile_name="DatabricksS3AccessProfile", # Use this friendly name in the Databricks UI
            roles=[databricks_role.role_name],
            path='/'
        )
        
        # 5. Output the ARNs and Names needed for Databricks setup

        # Output S3 Bucket Name and Path (Needed for Unity Catalog setup)
        CfnOutput(
            self, "DeltaLakeBucketNameOutput",
            value=delta_lake_bucket.bucket_name,
            description="The S3 Bucket name for the Delta Lake storage."
        )
        CfnOutput(
            self, "DeltaLakeBasePathOutput",
            value=f"s3://{delta_lake_bucket.bucket_name}/unity-catalog/",
            description="The base S3 path for Unity Catalog External Location setup."
        )
        
        
        CfnOutput(
            self, "DatabricksInstanceProfileARN",
            value=instance_profile.attr_arn,
            description="ARN to add to Databricks in the console -> Instance Profiles."
        )

        CfnOutput(
            self, "DatabricksInstanceProfileName",
            value=instance_profile.instance_profile_name,
            description="Name to select when configuring Databricks cluster."
        )

        CfnOutput(
            self, "KinesisStreamNameOutput",
            value=iot_stream.stream_name,
            description="The Kinesis Stream name for the data producer."
        )
